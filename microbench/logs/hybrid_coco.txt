/home/aips/miniconda3/envs/vlm_hybrid/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
WARNING:root:Incompatible keys: <All keys matched successfully>
/home/aips/miniconda3/envs/vlm_hybrid/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading thresholds from /home/aips/vlm/checkpoints/threshold_checkpoints/ViT-L-14-336-tome-72out.pth
Model cfg: {'embed_dim': 768, 'quick_gelu': True, 'vision_cfg': {'image_size': 336, 'layers': 24, 'width': 1024, 'patch_size': 14, 'tome_cfg': {'r_total': 504, 'merge_mode': 'batch_level', 'r_schedule': 'constant'}}, 'text_cfg': {'context_length': 77, 'vocab_size': 49408, 'width': 768, 'heads': 12, 'layers': 12}}
creating ToMEOpenAIVisionTransformer using ToMe config: merge_mode=batch_level, r_total=504, r_schedule=constant, {}
set total avg remove token nums each layer as:  [21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21]
merge mode:  batch_level
attn_pool: None
pool_type: tok
Loaded DyMU thresholds: 24 layers
Loading model: liuhaotian/llava-v1.5-7b
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:08<00:08,  8.87s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.06s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.63s/it]
WARNING:root:Incompatible keys: <All keys matched successfully>
‚ö†Ô∏è  Vanilla Vision Tower detected (CLIPVisionTower). Swapping to CLIPVisionTowerToMe for DyMU...
CLIPVisionTowerToMe select_layer: -2, select_feature: patch
Loading ToME vision transformer model name: ViT-L-14-336-tome-72out
ToME vision transformer updated args: {'pretrained': 'openai', 'pretrained_origin_tag': 'openai', 'merge_mode': 'batch_level', 'repeat_merged_tokens': False, 'r_total': 504, 'specified_thresholds': [0.99609375, 0.97265625, 0.94140625, 0.91015625, 0.87890625, 0.87890625, 0.87890625, 0.84765625, 0.84765625, 0.87890625, 0.84765625, 0.84765625, 0.87890625, 0.87890625, 0.91015625, 0.91015625, 0.91015625, 0.91015625, 0.94140625, 0.87890625, 0.87890625, 0.97265625, 0.99609375, 0.81640625]}
Setting 'repeat_merged_tokens' to: False
Model cfg: {'embed_dim': 768, 'quick_gelu': True, 'vision_cfg': {'image_size': 336, 'layers': 24, 'width': 1024, 'patch_size': 14, 'tome_cfg': {'r_total': 504, 'merge_mode': 'batch_level', 'r_schedule': 'constant'}}, 'text_cfg': {'context_length': 77, 'vocab_size': 49408, 'width': 768, 'heads': 12, 'layers': 12}, 'pretrained_origin_tag': 'openai', 'merge_mode': 'batch_level', 'repeat_merged_tokens': False, 'r_total': 504, 'specified_thresholds': [0.99609375, 0.97265625, 0.94140625, 0.91015625, 0.87890625, 0.87890625, 0.87890625, 0.84765625, 0.84765625, 0.87890625, 0.84765625, 0.84765625, 0.87890625, 0.87890625, 0.91015625, 0.91015625, 0.91015625, 0.91015625, 0.94140625, 0.87890625, 0.87890625, 0.97265625, 0.99609375, 0.81640625]}
creating ToMEOpenAIVisionTransformer using ToMe config: merge_mode=batch_level, r_total=504, r_schedule=constant, {'pretrained_origin_tag': 'openai', 'repeat_merged_tokens': False, 'specified_thresholds': [0.99609375, 0.97265625, 0.94140625, 0.91015625, 0.87890625, 0.87890625, 0.87890625, 0.84765625, 0.84765625, 0.87890625, 0.84765625, 0.84765625, 0.87890625, 0.87890625, 0.91015625, 0.91015625, 0.91015625, 0.91015625, 0.94140625, 0.87890625, 0.87890625, 0.97265625, 0.99609375, 0.81640625]}
set total avg remove token nums each layer as:  [21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21]
merge mode:  batch_level
attn_pool: None
pool_type: tok
WARNING: Checkpoint does not have visual.transformer.resblocks.0.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.0.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.1.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.1.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.2.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.2.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.3.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.3.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.4.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.4.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.5.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.5.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.6.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.6.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.7.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.7.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.8.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.8.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.9.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.9.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.10.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.10.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.11.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.11.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.12.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.12.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.13.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.13.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.14.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.14.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.15.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.15.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.16.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.16.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.17.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.17.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.18.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.18.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.19.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.19.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.20.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.20.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.21.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.21.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.22.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.22.threshold, using specified_thresholds.
WARNING: Checkpoint does not have visual.transformer.resblocks.23.threshold, resetting it to 1.0.
WARNING: Skipping loading of pretrained threshold visual.transformer.resblocks.23.threshold, using specified_thresholds.
Model cfg: {'embed_dim': 768, 'quick_gelu': True, 'vision_cfg': {'image_size': 336, 'layers': 24, 'width': 1024, 'patch_size': 14, 'tome_cfg': {'r_total': 504, 'merge_mode': 'batch_level', 'r_schedule': 'constant'}}, 'text_cfg': {'context_length': 77, 'vocab_size': 49408, 'width': 768, 'heads': 12, 'layers': 12}}
creating ToMEOpenAIVisionTransformer using ToMe config: merge_mode=batch_level, r_total=504, r_schedule=constant, {}
set total avg remove token nums each layer as:  [21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21]
merge mode:  batch_level
attn_pool: None
pool_type: tok
WARNING: Checkpoint does not have visual.transformer.resblocks.0.threshold, resetting it to 1.0.
WARNING:root:Incompatible keys: <All keys matched successfully>
WARNING: Checkpoint does not have visual.transformer.resblocks.1.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.2.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.3.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.4.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.5.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.6.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.7.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.8.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.9.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.10.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.11.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.12.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.13.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.14.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.15.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.16.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.17.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.18.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.19.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.20.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.21.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.22.threshold, resetting it to 1.0.
WARNING: Checkpoint does not have visual.transformer.resblocks.23.threshold, resetting it to 1.0.
tome preprocess cfg: PreprocessCfg(size=(336, 336), mode='RGB', mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711), interpolation='bicubic', resize_mode='shortest', fill_color=0)
‚úÖ DyMU Vision Tower injected. Thresholds: True
üöÄ FastV enabled: K=72, R=3, image_len=72
Loaded 500 items from perf_data/coco_val_500.json
Limiting to 1 samples
Hybrid Benchmark:   0%|          | 0/1 [00:00<?, ?it/s]/home/aips/miniconda3/envs/vlm_hybrid/lib/python3.10/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
[Profiling] VisionStep: 678.6387 ms
[DEBUG] After DyMU: 152 visual tokens per image
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 219, 4096]), attention_mask.shape=torch.Size([1, 1, 219, 219])
[DEBUG] LlamaModel.forward (Layer 0): seq_len=219, mask_shape=torch.Size([1, 1, 219, 219])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 219, 128]), key_states.shape=torch.Size([1, 32, 219, 128]), attention_mask.shape=torch.Size([1, 1, 219, 219])
[DEBUG] LlamaModel.forward (Layer 1): seq_len=219, mask_shape=torch.Size([1, 1, 219, 219])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 219, 128]), key_states.shape=torch.Size([1, 32, 219, 128]), attention_mask.shape=torch.Size([1, 1, 219, 219])
[DEBUG] LlamaModel.forward (Layer 2): seq_len=219, mask_shape=torch.Size([1, 1, 219, 219])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 219, 128]), key_states.shape=torch.Size([1, 32, 219, 128]), attention_mask.shape=torch.Size([1, 1, 219, 219])
[DEBUG] FastV Pruning (Layer 3): 107 tokens kept (Rank set to 72)
[DEBUG] LlamaModel.forward (Layer 3): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 4): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 5): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 6): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 7): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 8): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 9): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 10): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 11): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 12): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 13): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 14): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 15): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 16): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 17): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 18): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 19): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 20): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 21): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 22): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 23): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 24): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 25): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 26): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 27): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 28): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 29): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 30): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward (Layer 31): seq_len=107, mask_shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 107, 128]), key_states.shape=torch.Size([1, 32, 107, 128]), attention_mask.shape=torch.Size([1, 1, 107, 107])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 108])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 109])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 110])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 111])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 112])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 113])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 114])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 115])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 116])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 117])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 118])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 119])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 120])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 121])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 122])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 123])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 124])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 125])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 126])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 127])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 128])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 129])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 130])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 131])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 132])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 133])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 134])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 135])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 136])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 137])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 138])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 139])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 140])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 141])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 142])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 143])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 144])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 145])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 146])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 147])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 148])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 149])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 150])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 151])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 152])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 153])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 154])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 155])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 156])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 157])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 158])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 159])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 160])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 161])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 162])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 163])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 164])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 165])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 166])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 167])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 168])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 169])
[DEBUG] LlamaModel.forward: hidden_states.shape=torch.Size([1, 1, 4096]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
[DEBUG] LlamaAttention.forward: query_states.shape=torch.Size([1, 32, 1, 128]), key_states.shape=torch.Size([1, 32, 1, 128]), attention_mask.shape=torch.Size([1, 1, 1, 170])
Hybrid Benchmark:   0%|          | 0/1 [00:03<?, ?it/s, E2E=2.67s, TPS=24.8]Hybrid Benchmark: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.37s/it, E2E=2.67s, TPS=24.8]Hybrid Benchmark: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.37s/it, E2E=2.67s, TPS=24.8]
[Profiling] TTFT (Vision+Prefill): 89.4802 ms
[Profiling] LLM_Decode: 41.6739 ms
[Profiling] LLM_Decode: 34.8148 ms
[Profiling] LLM_Decode: 38.8930 ms
[Profiling] LLM_Decode: 34.6587 ms
[Profiling] LLM_Decode: 34.8964 ms
[Profiling] LLM_Decode: 34.5230 ms
[Profiling] LLM_Decode: 33.8318 ms
[Profiling] LLM_Decode: 33.9358 ms
[Profiling] LLM_Decode: 33.7620 ms
[Profiling] LLM_Decode: 33.7939 ms
[Profiling] LLM_Decode: 33.7918 ms
[Profiling] LLM_Decode: 33.9139 ms
[Profiling] LLM_Decode: 33.9482 ms
[Profiling] LLM_Decode: 38.5063 ms
[Profiling] LLM_Decode: 33.9391 ms
[Profiling] LLM_Decode: 33.8137 ms
[Profiling] LLM_Decode: 33.7722 ms
[Profiling] LLM_Decode: 33.8123 ms
[Profiling] LLM_Decode: 33.8488 ms
[Profiling] LLM_Decode: 33.8027 ms
[Profiling] LLM_Decode: 33.7622 ms
[Profiling] LLM_Decode: 33.8414 ms
[Profiling] LLM_Decode: 34.3060 ms
[Profiling] LLM_Decode: 38.7890 ms
[Profiling] LLM_Decode: 33.7410 ms
[Profiling] LLM_Decode: 33.9711 ms
[Profiling] LLM_Decode: 33.8926 ms
[Profiling] LLM_Decode: 33.8848 ms
[Profiling] LLM_Decode: 34.2181 ms
[Profiling] LLM_Decode: 33.9212 ms
[Profiling] LLM_Decode: 34.0586 ms
[Profiling] LLM_Decode: 33.7579 ms
[Profiling] LLM_Decode: 33.9079 ms
[Profiling] LLM_Decode: 33.8402 ms
[Profiling] LLM_Decode: 37.5414 ms
[Profiling] LLM_Decode: 33.9091 ms
[Profiling] LLM_Decode: 34.0505 ms
[Profiling] LLM_Decode: 443.1291 ms
[Profiling] LLM_Decode: 34.1711 ms
[Profiling] LLM_Decode: 34.0784 ms
[Profiling] LLM_Decode: 36.3736 ms
[Profiling] LLM_Decode: 33.9470 ms
[Profiling] LLM_Decode: 34.0486 ms
[Profiling] LLM_Decode: 34.0347 ms
[Profiling] LLM_Decode: 33.8919 ms
[Profiling] LLM_Decode: 33.8998 ms
[Profiling] LLM_Decode: 34.0300 ms
[Profiling] LLM_Decode: 34.5860 ms
[Profiling] LLM_Decode: 33.8891 ms
[Profiling] LLM_Decode: 33.9322 ms
[Profiling] LLM_Decode: 33.8717 ms
[Profiling] LLM_Decode: 33.9150 ms
[Profiling] LLM_Decode: 34.0497 ms
[Profiling] LLM_Decode: 33.9243 ms
[Profiling] LLM_Decode: 34.2588 ms
[Profiling] LLM_Decode: 34.1840 ms
[Profiling] LLM_Decode: 33.8292 ms
[Profiling] LLM_Decode: 33.8798 ms
[Profiling] LLM_Decode: 33.8476 ms
[Profiling] LLM_Decode: 33.9017 ms
[Profiling] LLM_Decode: 33.8776 ms
[Profiling] LLM_Decode: 34.2023 ms
[Profiling] LLM_Decode: 33.9146 ms

===== Performance Report (Hybrid: DyMU + FastV) =====
Dataset: COCO_VAL
Total Samples: 1
E2E Latency (s): P50=2.6691, P95=2.6691
TTFT (s):        P50=0.0895, P95=0.0895, Avg=0.0895
TBT (s):         P50=0.0403, P95=0.0403, Avg=0.0403
Decode TPS:      P50=24.82, Avg=24.82
Peak VRAM (MB):  Max=14303.83
Avg Output Len:  65.0 tokens
Avg Input Len:   68.0 tokens
==================================================
Report saved to reports/hybrid_coco.json
