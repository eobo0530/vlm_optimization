TEST START
`torch_dtype` is deprecated! Use `dtype` instead!
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
CLIPVisionTowerToMe select_layer: -2, select_feature: patch
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]
Retrieved kwargs for ToME vision tower from config: {}
Loading ToME vision transformer model name: ViT-L-14-336-tome-72out
ToME vision transformer updated args: {}
Setting 'repeat_merged_tokens' to: False
Model cfg: {'embed_dim': 768, 'quick_gelu': True, 'vision_cfg': {'image_size': 336, 'layers': 24, 'width': 1024, 'patch_size': 14, 'tome_cfg': {'r_total': 504, 'merge_mode': 'batch_level', 'r_schedule': 'constant'}}, 'text_cfg': {'context_length': 77, 'vocab_size': 49408, 'width': 768, 'heads': 12, 'layers': 12}}
creating ToMEOpenAIVisionTransformer using ToMe config: merge_mode=batch_level, r_total=504, r_schedule=constant, {}
set total avg remove token nums each layer as:  [21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21]
merge mode:  batch_level
attn_pool: None
pool_type: tok
Error: pretrained_origin_tag not found in {}
TEST END
